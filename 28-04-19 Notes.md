# A Few Useful Things to Know about Machine Learning (Notes)

## Terminology:

* __Classifier__ - A system that inputs a vector of discrete/continuous _feature values_ and outputs a single discrete value called a _class_ or _category_
* __Learner__ - A learner inputs a _training set_ of _examples_ and outputs a _classifier_. 
* __Hypothesis Space__ - The set of _classifiers_ a _learner_ can possibly learn. Anything outside the _hypothesis space_ cannot be learned. 
* __Objective Function/Scoring Function__ - Used to distinguish good classifiers from bad ones. E.g, mean average errors.
* __Overfitting__ - A model that models the training data too well, including details and noise, such that it negatively impacts the model on new data. 
* __Underfitting__ - A model that can neither model the training data nor new data. 
* __Reguarization Term__ - A method to combat overfitting. Penalizes classifiers with with more structure favoring smaller ones with less room to overfit. 
* __Model Ensembles__ - Combining many variations of learners. 
* __Bagging__ - A method of generating random variations of the training set by resampling, learn a classifier on each, and combine the results by voting. 
* __Boosting__ - Training examples have weights and these are varied so that each new classifier focuses on the examples the previous ones tended to get wrong. In _stacking_, the outputs of individual classifiers become the inputs of a "higher-level" learner that figures out how best to combine them. 

## Learning = Representation + Evaluation + Optimization

### Representation
#### Examples
* Instances
  * K-Nearest neighbours
  * Support vector machines
* Hyperplanes
  * Naive Bayes
  * Logistic Regression
* Decision Trees
* Set of rules
  * Propositional Rules
  * Logic programs
* Neural networks
* Graphical models
  * Bayesian networks
  * Conditional random fields

### Evaluation
#### Examples
* Accuracy/Error rate
* Precision and recall
* Squared error
* Likelihood
* Posterior probability
* Information gain
* K-L Divergence 
* Cost/Utility
* Margin

### Optimization
#### Examples
* Combinatorial Optimisation
  * Greedy Search
  * Beam Search
  * Branch-and-bound
* Continous Optimisation
  * Unconstrained
    * Gradient descent
    * Conjugate gradient
    * Quasi-Newton methods
  * Constrained
    * Linear programming
    * Quadratic programming

## Curse of Dimensionality

To add...

## Theoretical Guarantees

To add...
