# A Few Useful Things to Know about Machine Learning (Notes)

## Terminology:

* Classifier - A system that inputs a vector of discrete/continuous _feature values_ and outputs a single discrete value called a _class_ or _category_
* Learner - A learner inputs a _training set_ of _examples_ and outputs a _classifier_. 
* Hypothesis Space - The set of _classifiers_ a _learner_ can possibly learn. Anything outside the _hypothesis space_ cannot be learned. 
* Objective Function/Scoring Function - Used to distinguish good classifiers from bad ones. E.g, mean average errors.
* Overfitting - A model that models the training data too well, including details and noise, such that it negatively impacts the model on new data. 
* Underfitting - A model that can neither model the training data nor new data. 
* Reguarization Term - A method to combat overfitting. Penalizes classifiers with with more structure favoring smaller ones with less room to overfit. 
* Model Ensembles - Combining many variations of learners. 
* Bagging - A method of generating random variations of the training set by resampling, learn a classifier on each, and combine the results by voting. 
* Boosting - Training examples have weights and these are varied so that each new classifier focuses on the examples the previous ones tended to get wrong. In _stacking_, the outputs of individual classifiers become the inputs of a "higher-level" learner that figures out how best to combine them. 

## Learning = Representation + Evaluation + Optimization



## Curse of Dimensionality

## Theoretical Guarantees

